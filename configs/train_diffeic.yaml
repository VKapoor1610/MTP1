data:
  target: dataset.data_module.DataModule
  params:
    # Path to training set configuration file.
    train_config: /kaggle/working/MTP1/configs/dataset/lic_train.yaml
    # Path to validation set configuration file.
    val_config: /kaggle/working/MTP1/configs/dataset/lic_valid.yaml
model:
  # You can set learning rate in the following configuration file.
  config: /kaggle/working/MTP1/configs/model/diffeic.yaml
  # Path to the checkpoints or weights you want to resume. At the begining, 
  resume: ~

lightning:
  seed: 231
  
  # trainer:
  #   accelerator: ddp
  #   precision: 32
  #   # Indices of GPUs used for training.
  #   gpus: [0]
  #   # Path to save logs and checkpoints.
  #   default_root_dir: ./logs/1_2_1
  #   # Max number of training steps (batches).
  #   max_steps: 300001
  #   # Validation frequency in terms of training steps.
  #   val_check_interval: 1
  #   log_every_n_steps: 100
  #   # Accumulate gradients from multiple batches so as to increase batch size.
  #   accumulate_grad_batches: 1

  trainer:
    accelerator: gpu  # Use 'gpu' instead of 'ddp' for single GPU training
    precision: 16  # Enable mixed precision (16-bit) training to reduce memory usage
    gpus: [0]  # Keep GPU 0 for training
    default_root_dir: ./logs/1_2_1  # Path to save logs and checkpoints
    max_steps: 300001  # Max number of training steps
    val_check_interval: 1.0  # Validate after every epoch (adjust if you prefer validation frequency by batches)
    log_every_n_steps: 100  # Log every 100 steps
    accumulate_grad_batches: 1  # Gradient accumulation, no changes needed her
  
  callbacks:
    - target: model.callbacks.ImageLogger
      params:
        # Log frequency of image logger.
        log_every_n_steps: 1000
        log_start_step: 0
        max_images_each_step: 2
        log_images_kwargs: ~

    - target: model.callbacks.ModelCheckpoint
      params:
        # Frequency of saving checkpoints.
        every_n_train_steps: 5000
        save_top_k: -1
        filename: "{step}"
